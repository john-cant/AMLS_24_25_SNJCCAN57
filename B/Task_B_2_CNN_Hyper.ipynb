{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgV_pyyf--49"
   },
   "source": [
    "\n",
    "\n",
    "# AMLS Assignment\n",
    "## Task B_2: CNN Hyper on BloodMNIST Dataset\n",
    "\n",
    "Explore CNN hyperparameter set selection for the BloodMNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "928tj4hm_HRl"
   },
   "source": [
    "## Import libraries\n",
    "The required libraries for this notebook are sklearn, tensorflow, numpy and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZR9HIUUN-601"
   },
   "outputs": [],
   "source": [
    "## first enable autoreload during development so latest (new) version local code library is reloaded on execution \n",
    "## can be commented out when local code development is complete to avoid any overhead\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "## import libraries\n",
    "import io\n",
    "import time                          ## to enable delays between file saves\n",
    "import numpy as np                   ## array manipulation\n",
    "import matplotlib.pyplot as plt      ## for graphing\n",
    "## import tensorflow\n",
    "import tensorflow as tf              ## tensor, model functions\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, SparseCategoricalCrossentropy\n",
    "## local code library, - developed for these specific tasks and includes all references to MedMNIST specific library\n",
    "import AMLS_common as ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set base parameters\n",
    "Including hyperparameter lists and data set specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter set combinations: 8\n"
     ]
    }
   ],
   "source": [
    "## set up lists and parameters\n",
    "hyper_list    = []\n",
    "run_list      = []\n",
    "batch_size    = 128                 ## batch size for datasets\n",
    "patience      = 3                   ## number of overfitting epochs before terminating\n",
    "threshold     = 0.1                 ## overfitting threshold\n",
    "## control and environment (e.g. verbose) parameters\n",
    "filebase      = \"metrics/\"          ## folder under current directory to store output files\n",
    "verbose       = 0                   ## to control whether additional in process information is printed\n",
    "\n",
    "## use these lists of values to grid test hyper parameter sensitivity                \n",
    "epochs_list   = [15,20]                             ## set of epochs to run for\n",
    "filter_list   = [64,128]                                ## main filter sizes to use\n",
    "ks_list       = [3]                                 ## kernel size\n",
    "lr_list       = [0.01]                        ## learning rates\n",
    "ly_list       = [4,5]                               ## number of covolution layers\n",
    "dr_list       = [0.25]                          ## selected dropout rates\n",
    "st_list       = [2]                               ## stride list\n",
    "loss_list     = ['sparse_categorical_crossentropy'] ## loss functions to use\n",
    "optimise_list = ['Adam']                            ## optimisation functions\n",
    "padding       = \"same\"\n",
    "## now set up the required hyperparameter sets\n",
    "for lr in lr_list:\n",
    "    for ks in ks_list:\n",
    "        for ep in epochs_list:\n",
    "            for fi in filter_list:\n",
    "                for ly in ly_list:\n",
    "                    for dr in dr_list:\n",
    "                        for st in st_list:\n",
    "                            for op in optimise_list:\n",
    "                                for ls in loss_list:\n",
    "                                    parameter_set = ac.HyperParameters(learning_rate=lr, \n",
    "                                                                       kernel_size=ks, \n",
    "                                                                       num_epochs=ep, \n",
    "                                                                       num_filter=fi,\n",
    "                                                                       layers=ly,\n",
    "                                                                       dropout_rate=dr,\n",
    "                                                                       strides=st,\n",
    "                                                                       padding=padding,\n",
    "                                                                       optimise=op,\n",
    "                                                                       loss=ls)          \n",
    "                                    hyper_list.append([parameter_set])        \n",
    "## reshape parameters into a test grid that can be read using for loop\n",
    "hyper_grid = [hp for sublist in hyper_list for hp in sublist]\n",
    "print(\"Hyperparameter set combinations:\",len(hyper_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LY2oUxSh_Nbk"
   },
   "source": [
    "## Load and preprocess the BloodMNIST Data\n",
    "We load the dataset using the specifically developed common AMLS library. Uses default batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PeebuSr_MAs",
    "outputId": "8a71101f-13e8-4381-eaff-f3bd4887b994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\johnc\\.medmnist\\bloodmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\johnc\\.medmnist\\bloodmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\johnc\\.medmnist\\bloodmnist.npz\n"
     ]
    }
   ],
   "source": [
    "## Loading the data file using common MedMINST loader\n",
    "data_flag  = 'bloodmnist'                            ## defines which dataset to load\n",
    "result_set = ac.medMNIST_load(data_flag,batch_size)  ## batch size currently hardwired\n",
    "\n",
    "## check that the loader returned data correctly and then split out\n",
    "if result_set != []:\n",
    "    train_dataset = result_set[0]               ## training set\n",
    "    test_dataset  = result_set[1]               ## test set\n",
    "    val_dataset   = result_set[2]               ## validation set\n",
    "\n",
    "if verbose == 1:\n",
    "    print(\"\\nSummary metrics for train_dataset\")\n",
    "    print(\"type:\",type(val_dataset))\n",
    "    print(\"length:\",len(val_dataset))\n",
    "    print(\"shape:\",val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model\n",
    "\n",
    "Using each Hyperparameter in turn from a superset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 / 8 with HyperParameters(learning_rate=0.01, kernel_size=3, num_epochs=15, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=64, strides=2, padding='same', dropout_rate=0.25, layers=4, default_activation='relu')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 15/15 [29:30<00:00, 118.02s/epoch, loss=0.243, acc=0.91, val_loss=0.233, val_acc=0.919] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2 / 8 with HyperParameters(learning_rate=0.01, kernel_size=3, num_epochs=15, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=64, strides=2, padding='same', dropout_rate=0.25, layers=5, default_activation='relu')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  13%|█▎        | 2/15 [03:54<25:28, 117.57s/epoch, loss=0.852, acc=0.675, val_loss=0.999, val_acc=0.627]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overfitting detected at epoch 2: Loss Gap = 0.1463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  27%|██▋       | 4/15 [07:42<21:09, 115.39s/epoch, loss=0.671, acc=0.759, val_loss=0.828, val_acc=0.716]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overfitting detected at epoch 4: Loss Gap = 0.1571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 15/15 [29:56<00:00, 119.77s/epoch, loss=0.269, acc=0.901, val_loss=0.301, val_acc=0.898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3 / 8 with HyperParameters(learning_rate=0.01, kernel_size=3, num_epochs=15, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=128, strides=2, padding='same', dropout_rate=0.25, layers=4, default_activation='relu')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 15/15 [1:29:38<00:00, 358.54s/epoch, loss=0.188, acc=0.932, val_loss=0.26, val_acc=0.921] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 / 8 with HyperParameters(learning_rate=0.01, kernel_size=3, num_epochs=15, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=128, strides=2, padding='same', dropout_rate=0.25, layers=5, default_activation='relu')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   7%|▋         | 1/15 [14:41<3:25:40, 881.49s/epoch, loss=1.54, acc=0.403, val_loss=0.947, val_acc=0.633]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 93\u001b[0m\n\u001b[0;32m     88\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mitem\u001b[38;5;241m.\u001b[39moptimise,                                                   \n\u001b[0;32m     89\u001b[0m               loss\u001b[38;5;241m=\u001b[39mitem\u001b[38;5;241m.\u001b[39mloss,\n\u001b[0;32m     90\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m## Fit the model\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtqdm_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstop_overfit_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m## Save results to files\u001b[39;00m\n\u001b[0;32m    101\u001b[0m run_list\u001b[38;5;241m.\u001b[39mappend(ac\u001b[38;5;241m.\u001b[39mhyper_process(history,summary_content,item))\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\AMLS2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:108\u001b[0m, in \u001b[0;36menable_multi_worker.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_method_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    107\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_multi_worker_mode():  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m   \u001b[38;5;66;03m# Running inside `run_distribute_coordinator` already.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dc_context\u001b[38;5;241m.\u001b[39mget_current_worker_context():\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\AMLS2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1098\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraceContext\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1093\u001b[0m     graph_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1094\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1095\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1096\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size):\n\u001b[0;32m   1097\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1098\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1100\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\AMLS2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:780\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    779\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 780\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tracing_count()\n\u001b[0;32m    783\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\AMLS2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:807\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    804\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    805\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    806\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 807\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    809\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    810\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    811\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\AMLS2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2829\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2828\u001b[0m   graph_function, args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filtered_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\AMLS2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1843\u001b[0m, in \u001b[0;36mConcreteFunction._filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_filtered_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, kwargs, cancellation_manager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1828\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Executes the function, filtering arguments from the Python function.\u001b[39;00m\n\u001b[0;32m   1829\u001b[0m \n\u001b[0;32m   1830\u001b[0m \u001b[38;5;124;03m  Objects aside from Tensors, CompositeTensors, and Variables are ignored.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1841\u001b[0m \u001b[38;5;124;03m    `args` and `kwargs`.\u001b[39;00m\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1843\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1844\u001b[0m \u001b[43m      \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1845\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1846\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mresource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBaseResourceVariable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1847\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\AMLS2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1923\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1918\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1919\u001b[0m     pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_TapeSetPossibleGradientTypes(args))\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m _POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1921\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1922\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1923\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1924\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1925\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m     args,\n\u001b[0;32m   1927\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1928\u001b[0m     executing_eagerly)\n\u001b[0;32m   1929\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\AMLS2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:545\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    544\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 545\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    552\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    553\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    554\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    558\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\AMLS2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## set up the variables to keep track of the hyperparameter combinations\n",
    "iterations      = len(hyper_grid)                                     ## number of hyperparameter sets\n",
    "countie         = 0                                                   ## interim count for iterations in loop\n",
    "stop_overfit_cb = ac.StopOverfittingCallback(patience, threshold)     ## initialise overfitting callback\n",
    "## Create instances of the dataclass from the list\n",
    "for item in hyper_grid:\n",
    "    countie += 1\n",
    "    ## Define the model which is then run for all hyperparameters in set\n",
    "    print(\"Run\",countie,\"/\",iterations,\"with\",item)\n",
    "    ## initialise tqdm callback\n",
    "    tqdm_callback = ac.TqdmEpochProgress(total_epochs=item.num_epochs)\n",
    "    \n",
    "    ## Simple CNN model to support hyperparameter selection\n",
    "    ## added desired number of layers\n",
    "    if item.layers == 3:\n",
    "            model = Sequential([\n",
    "                Conv2D(item.num_filter*4, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation,\\\n",
    "                       input_shape=(28, 28, 3)),                                             ## Input layer with larger num filter\n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## Second part of convolution layer \n",
    "                MaxPooling2D((2, 2),strides=item.strides),                                   ## Combined with pooling\n",
    "                Dropout(item.dropout_rate),                                                  ## Initial dropout to reduce overfitting\n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## Another convolution layer \n",
    "                MaxPooling2D((2, 2),strides=item.strides),                                   ## Combined with pooling\n",
    "                Flatten(),                                                                   ## Flatten\n",
    "                Dense(item.num_filter*8,activation=item.default_activation),\n",
    "                Dropout(item.dropout_rate*2),                                                ## Added larger dropout to reduce overfitting\n",
    "                Dense(8, activation='softmax')                                               ## Output layer for 8 types \n",
    "            ])\n",
    "        \n",
    "    if item.layers == 4:\n",
    "            model = Sequential([\n",
    "                Conv2D(item.num_filter*4, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation,\\\n",
    "                       input_shape=(28, 28, 3)),                                             ## Input layer with larger num filter\n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## Second part of convolution layer \n",
    "                MaxPooling2D((2, 2),strides=item.strides),                                   ## Combined with pooling\n",
    "                Dropout(item.dropout_rate),                                                  ## Initial dropout to reduce overfitting\n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## Another convolution layer \n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## With added convolution layer \n",
    "                MaxPooling2D((2, 2),strides=item.strides),                                   ## Combined with pooling\n",
    "                Flatten(),                                                                   ## Flatten\n",
    "                Dense(item.num_filter*8,activation=item.default_activation),\n",
    "                Dropout(item.dropout_rate*2),                                                ## Added larger dropout to reduce overfitting\n",
    "                Dense(8, activation='softmax')                                               ## Output layer for 8 types \n",
    "            ])\n",
    "    \n",
    "    if item.layers == 5:\n",
    "            model = Sequential([\n",
    "                Conv2D(item.num_filter*4, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation,\\\n",
    "                       input_shape=(28, 28, 3)),                                             ## Input layer with larger num filter\n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## Second part of convolution layer \n",
    "                MaxPooling2D((2, 2),strides=item.strides),                                   ## Combined with pooling\n",
    "                Dropout(item.dropout_rate),                                                  ## Initial dropout to reduce overfitting\n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## Another convolution layer \n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## With added convolution layer \n",
    "                MaxPooling2D((2, 2),strides=item.strides),                                   ## Again reduce the features\n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## Another convolution layer \n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## With added convolution layer \n",
    "                MaxPooling2D((2, 2),strides=item.strides),                                   ## Combined with pooling\n",
    "                Flatten(),                                                                   ## Flatten\n",
    "                Dense(item.num_filter*8,activation=item.default_activation),\n",
    "                Dropout(item.dropout_rate*2),                                                ## Added larger dropout to reduce overfitting\n",
    "                Dense(8, activation='softmax')                                               ## Output layer for 8 types \n",
    "            ])\n",
    "\n",
    "    if verbose == 1:\n",
    "        print(model.summary())\n",
    "        \n",
    "    ## Redirect the summary output to a string\n",
    "    summary_string  = io.StringIO()\n",
    "    model.summary(print_fn=lambda x: summary_string.write(x + \"\\n\"))\n",
    "    summary_content = summary_string.getvalue()\n",
    "    summary_string.close()\n",
    "\n",
    "    ## Compile the model\n",
    "    model.compile(optimizer=item.optimise,                                                   \n",
    "                  loss=item.loss,\n",
    "                  metrics='acc')\n",
    "\n",
    "    ## Fit the model\n",
    "    history = model.fit(train_dataset,\n",
    "                        validation_data=val_dataset, \n",
    "                        epochs=item.num_epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        verbose=0,\n",
    "                        callbacks = [tqdm_callback,stop_overfit_cb])\n",
    "    \n",
    "    ## Save results to files\n",
    "    run_list.append(ac.hyper_process(history,summary_content,item))\n",
    "\n",
    "print(\"Hyperparameter test run complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get best hyperparameter sets and both print them out and save them to parameter files that can be fed to Tune model runs\n",
    "run_df,best_run,best_run2,best_run3 = ac.analyse_run(run_list,\" \",filebase)\n",
    "print(\"\\nRun satisfying both smallest min_loss and largest max_acc:\")\n",
    "if len(best_run) > 0:\n",
    "    ac.process_best_run(best_run)\n",
    "print(\"\\n\")\n",
    "print(\"\\nRun with largest max_acc that is plateau or increasing:\")\n",
    "if len(best_run2) > 0:\n",
    "    ac.process_best_run(best_run2)\n",
    "print(\"\\n\")\n",
    "print(\"\\nRun with smallest min_loss that is plateau or decreasing:\")\n",
    "if len(best_run3) > 0:\n",
    "    ac.process_best_run(best_run3)\n",
    "\n",
    "if len(run_df)>1:\n",
    "    feature_importance,coef = ac.analyse_hyperparameters(run_df)\n",
    "    print(\"\\nImpact of Hyperparameters on Accuracy (from Linear Regression):\")\n",
    "    print(coef)\n",
    "    print(\"\\nHyperparameter Importance for Accuracy (from Random Forest):\")\n",
    "    print(feature_importance)\n",
    "else:\n",
    "    print(\"\\n\")\n",
    "    print('Suppressed feature analysis as train set too small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "928tj4hm_HRl",
    "LY2oUxSh_Nbk",
    "pJ0M8Nzx_Tir",
    "V_0zJR_ZAD0-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
