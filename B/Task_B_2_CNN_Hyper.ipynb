{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgV_pyyf--49"
   },
   "source": [
    "\n",
    "\n",
    "# AMLS Assignment\n",
    "## Task B_2: CNN Hyper on BloodMNIST Dataset\n",
    "\n",
    "Explore CNN hyperparameter set selection for the BloodMNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "928tj4hm_HRl"
   },
   "source": [
    "## Import libraries\n",
    "The required libraries for this notebook are sklearn, tensorflow, numpy and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZR9HIUUN-601"
   },
   "outputs": [],
   "source": [
    "## first enable autoreload during development so latest (new) version local code library is reloaded on execution \n",
    "## can be commented out when local code development is complete to avoid any overhead\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "## import libraries\n",
    "import io\n",
    "import time                          ## to enable delays between file saves\n",
    "import numpy as np                   ## array manipulation\n",
    "import matplotlib.pyplot as plt      ## for graphing\n",
    "## import tensorflow\n",
    "import tensorflow as tf              ## tensor, model functions\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, SparseCategoricalCrossentropy\n",
    "## local code library, - developed for these specific tasks and includes all references to MedMNIST specific library\n",
    "import AMLS_common as ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set base parameters\n",
    "Including hyperparameter lists and data set specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up lists and parameters\n",
    "hyper_list    = []\n",
    "run_list      = []\n",
    "batch_size    = 128                 ## batch size for datasets\n",
    "patience      = 3                   ## number of overfitting epochs before terminating\n",
    "threshold     = 0.1                 ## overfitting threshold\n",
    "## control and environment (e.g. verbose) parameters\n",
    "filebase   = \"metrics/\"             ## folder under current directory to store output files\n",
    "verbose    = 0                      ## to control whether additional in process information is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter set combinations: 33\n"
     ]
    }
   ],
   "source": [
    "## use these lists of values to grid test hyper parameter sensitivity                \n",
    "epochs_list   = [15,20]                             ## set of epochs to run for\n",
    "filter_list   = [64]                                ## main filter sizes to use\n",
    "ks_list       = [3]                                 ## kernel size\n",
    "lr_list       = [0.01,0.005]                        ## learning rates\n",
    "ly_list       = [4,5]                               ## number of covolution layers\n",
    "dr_list       = [0.15,0.2]                          ## selected dropout rates\n",
    "st_list       = [1,2]                               ## stride list\n",
    "loss_list     = ['sparse_categorical_crossentropy'] ## loss functions to use\n",
    "optimise_list = ['Adam']                            ## optimisation functions\n",
    "padding       = \"same\"\n",
    "## now set up the required hyperparameter sets\n",
    "for lr in lr_list:\n",
    "    for ks in ks_list:\n",
    "        for ep in epochs_list:\n",
    "            for fi in filter_list:\n",
    "                for ly in ly_list:\n",
    "                    for dr in dr_list:\n",
    "                        for st in st_list:\n",
    "                            for op in optimise_list:\n",
    "                                for ls in loss_list:\n",
    "                                    parameter_set = ac.HyperParameters(learning_rate=lr, \n",
    "                                                                       kernel_size=ks, \n",
    "                                                                       num_epochs=ep, \n",
    "                                                                       num_filter=fi,\n",
    "                                                                       layers=ly,\n",
    "                                                                       dropout_rate=dr,\n",
    "                                                                       strides=st,\n",
    "                                                                       padding=padding,\n",
    "                                                                       optimise=op,\n",
    "                                                                       loss=ls)          \n",
    "                                    hyper_list.append([parameter_set])        \n",
    "## reshape parameters into a test grid that can be read using for loop\n",
    "hyper_grid = [hp for sublist in hyper_list for hp in sublist]\n",
    "print(\"Hyperparameter set combinations:\",len(hyper_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LY2oUxSh_Nbk"
   },
   "source": [
    "## Load and preprocess the BloodMNIST Data\n",
    "We load the dataset using the specifically developed common AMLS library. Uses default batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PeebuSr_MAs",
    "outputId": "8a71101f-13e8-4381-eaff-f3bd4887b994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\johnc\\.medmnist\\bloodmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\johnc\\.medmnist\\bloodmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\johnc\\.medmnist\\bloodmnist.npz\n"
     ]
    }
   ],
   "source": [
    "## Loading the data file using common MedMINST loader\n",
    "data_flag  = 'bloodmnist'                            ## defines which dataset to load\n",
    "result_set = ac.medMNIST_load(data_flag,batch_size)  ## batch size currently hardwired\n",
    "\n",
    "## check that the loader returned data correctly and then split out\n",
    "if result_set != []:\n",
    "    train_dataset = result_set[0]               ## training set\n",
    "    test_dataset  = result_set[1]               ## test set\n",
    "    val_dataset   = result_set[2]               ## validation set\n",
    "\n",
    "if verbose == 1:\n",
    "    print(\"\\nSummary metrics for train_dataset\")\n",
    "    print(\"type:\",type(val_dataset))\n",
    "    print(\"length:\",len(val_dataset))\n",
    "    print(\"shape:\",val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model\n",
    "\n",
    "Using each Hyperparameter in turn from a superset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 / 1 with HyperParameters(learning_rate=0.01, kernel_size=3, num_epochs=15, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=64, strides=1, padding='same', dropout_rate=0.15, layers=5, default_activation='relu')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  93%|█████████▎| 14/15 [45:20<03:08, 188.08s/epoch, loss=0.203, acc=0.924, val_loss=0.35, val_acc=0.886] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overfitting detected at epoch 14: Loss Gap = 0.1470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 15/15 [48:25<00:00, 193.73s/epoch, loss=0.208, acc=0.927, val_loss=0.265, val_acc=0.914]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter test run complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## set up the variables to keep track of the hyperparameter combinations\n",
    "iterations      = len(test_grid)                                      ## number of hyperparameter sets\n",
    "countie         = 0                                                   ## interim count for iterations in loop\n",
    "stop_overfit_cb = ac.StopOverfittingCallback(patience, threshold)     ## initialise overfitting callback\n",
    "## Create instances of the dataclass from the list\n",
    "for item in hyper_grid:\n",
    "    countie += 1\n",
    "    ## Define the model which is then run for all hyperparameters in set\n",
    "    print(\"Run\",countie,\"/\",iterations,\"with\",item)\n",
    "    ## initialise tqdm callback\n",
    "    tqdm_callback = ac.TqdmEpochProgress(total_epochs=item.num_epochs)\n",
    "    \n",
    "    ## Simple CNN model to support hyperparameter selection\n",
    "    ## added desired number of layers\n",
    "    if item.layers == 3:\n",
    "            model = Sequential([\n",
    "                Conv2D(item.num_filter*4, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation,\\\n",
    "                       input_shape=(28, 28, 3)),                                             ## Input layer with larger num filter\n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## Second part of convolution layer \n",
    "                MaxPooling2D((2, 2),strides=item.strides),                                   ## Combined with pooling\n",
    "                Dropout(item.dropout_rate),                                                  ## Initial dropout to reduce overfitting\n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## Another convolution layer \n",
    "                MaxPooling2D((2, 2),strides=item.strides),                                   ## Combined with pooling\n",
    "                Flatten(),                                                                   ## Flatten\n",
    "                Dense(item.num_filter*8,activation=item.default_activation),\n",
    "                Dropout(item.dropout_rate*2),                                                ## Added larger dropout to reduce overfitting\n",
    "                Dense(8, activation='softmax')                                               ## Output layer for 8 types \n",
    "            ])\n",
    "        \n",
    "    if item.layers == 4:\n",
    "            model = Sequential([\n",
    "                Conv2D(item.num_filter*4, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation,\\\n",
    "                       input_shape=(28, 28, 3)),                                             ## Input layer with larger num filter\n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## Second part of convolution layer \n",
    "                MaxPooling2D((2, 2),strides=item.strides),                                   ## Combined with pooling\n",
    "                Dropout(item.dropout_rate),                                                  ## Initial dropout to reduce overfitting\n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## Another convolution layer \n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## With added convolution layer \n",
    "                MaxPooling2D((2, 2),strides=item.strides),                                   ## Combined with pooling\n",
    "                Flatten(),                                                                   ## Flatten\n",
    "                Dense(item.num_filter*8,activation=item.default_activation),\n",
    "                Dropout(item.dropout_rate*2),                                                ## Added larger dropout to reduce overfitting\n",
    "                Dense(8, activation='softmax')                                               ## Output layer for 8 types \n",
    "            ])\n",
    "    \n",
    "    if item.layers == 5:\n",
    "            model = Sequential([\n",
    "                Conv2D(item.num_filter*4, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation,\\\n",
    "                       input_shape=(28, 28, 3)),                                             ## Input layer with larger num filter\n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## Second part of convolution layer \n",
    "                MaxPooling2D((2, 2),strides=item.strides),                                   ## Combined with pooling\n",
    "                Dropout(item.dropout_rate),                                                  ## Initial dropout to reduce overfitting\n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## Another convolution layer \n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## With added convolution layer \n",
    "                MaxPooling2D((2, 2),strides=item.strides),                                   ## Again reduce the features\n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## Another convolution layer \n",
    "                Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                       padding=item.padding,activation=item.default_activation),             ## With added convolution layer \n",
    "                MaxPooling2D((2, 2),strides=item.strides),                                   ## Combined with pooling\n",
    "                Flatten(),                                                                   ## Flatten\n",
    "                Dense(item.num_filter*8,activation=item.default_activation),\n",
    "                Dropout(item.dropout_rate*2),                                                ## Added larger dropout to reduce overfitting\n",
    "                Dense(8, activation='softmax')                                               ## Output layer for 8 types \n",
    "            ])\n",
    "\n",
    "    if verbose == 1:\n",
    "        print(model.summary())\n",
    "        \n",
    "    ## Redirect the summary output to a string\n",
    "    summary_string  = io.StringIO()\n",
    "    model.summary(print_fn=lambda x: summary_string.write(x + \"\\n\"))\n",
    "    summary_content = summary_string.getvalue()\n",
    "    summary_string.close()\n",
    "\n",
    "    ## Compile the model\n",
    "    model.compile(optimizer=item.optimise,                                                   \n",
    "                  loss=item.loss,\n",
    "                  metrics='acc')\n",
    "\n",
    "    ## Fit the model\n",
    "    history = model.fit(train_dataset,\n",
    "                        validation_data=val_dataset, \n",
    "                        epochs=item.num_epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        verbose=0,\n",
    "                        callbacks = [tqdm_callback,stop_overfit_cb])\n",
    "    \n",
    "    ## Save results to files\n",
    "    run_list.append(ac.hyper_process(history,summary_content,item))\n",
    "\n",
    "print(\"Hyperparameter test run complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run satisfying both smallest min_loss and largest max_acc:\n",
      "learning_rate : 0.01\n",
      "kernel_size : 3\n",
      "num_epochs : 15\n",
      "num_filter : 64\n",
      "strides : 1\n",
      "padding : same\n",
      "dropout_rate : 0.15\n",
      "layers : 5\n",
      "optimise : Adam\n",
      "loss : sparse_categorical_crossentropy\n",
      "default_activation : relu\n",
      "\n",
      "\n",
      "\n",
      "Run with largest max_acc that is plateau or increasing:\n",
      "learning_rate : 0.01\n",
      "kernel_size : 3\n",
      "num_epochs : 15\n",
      "num_filter : 64\n",
      "strides : 1\n",
      "padding : same\n",
      "dropout_rate : 0.15\n",
      "layers : 5\n",
      "optimise : Adam\n",
      "loss : sparse_categorical_crossentropy\n",
      "default_activation : relu\n",
      "\n",
      "\n",
      "\n",
      "Run with smallest min_loss that is plateau or decreasing:\n",
      "learning_rate : 0.01\n",
      "kernel_size : 3\n",
      "num_epochs : 15\n",
      "num_filter : 64\n",
      "strides : 1\n",
      "padding : same\n",
      "dropout_rate : 0.15\n",
      "layers : 5\n",
      "optimise : Adam\n",
      "loss : sparse_categorical_crossentropy\n",
      "default_activation : relu\n",
      "\n",
      "\n",
      "Suppressed feature analysis as train set too small\n"
     ]
    }
   ],
   "source": [
    "## Get best hyperparameter sets and both print them out and save them to parameter files that can be fed to Tune model runs\n",
    "run_df,best_run,best_run2,best_run3 = ac.analyse_run(run_list,\" \",filebase)\n",
    "print(\"\\nRun satisfying both smallest min_loss and largest max_acc:\")\n",
    "if len(best_run) > 0:\n",
    "    ac.process_best_run(best_run)\n",
    "print(\"\\n\")\n",
    "print(\"\\nRun with largest max_acc that is plateau or increasing:\")\n",
    "if len(best_run2) > 0:\n",
    "    ac.process_best_run(best_run2)\n",
    "print(\"\\n\")\n",
    "print(\"\\nRun with smallest min_loss that is plateau or decreasing:\")\n",
    "if len(best_run3) > 0:\n",
    "    ac.process_best_run(best_run3)\n",
    "\n",
    "if len(run_df)>1:\n",
    "    feature_importance,coef = ac.analyse_hyperparameters(run_df)\n",
    "    print(\"\\nImpact of Hyperparameters on Accuracy (from Linear Regression):\")\n",
    "    print(coef)\n",
    "    print(\"\\nHyperparameter Importance for Accuracy (from Random Forest):\")\n",
    "    print(feature_importance)\n",
    "else:\n",
    "    print(\"\\n\")\n",
    "    print('Suppressed feature analysis as train set too small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "928tj4hm_HRl",
    "LY2oUxSh_Nbk",
    "pJ0M8Nzx_Tir",
    "V_0zJR_ZAD0-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
