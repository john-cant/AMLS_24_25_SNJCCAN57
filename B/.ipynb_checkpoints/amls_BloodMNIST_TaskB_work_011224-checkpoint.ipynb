{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgV_pyyf--49"
   },
   "source": [
    "\n",
    "\n",
    "# AMLS Assignment Draft\n",
    "## Task B: CNN on BloodMNIST Dataset\n",
    "\n",
    "Explore CNN based classifiers on the BloodMNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "928tj4hm_HRl"
   },
   "source": [
    "## Import libraries\n",
    "The required libraries for this notebook are sklearn, copy, numpy and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZR9HIUUN-601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "## first enable autoreload during development so latest (new) version local code library is reloaded on execution \n",
    "## can be commented out when local code development not happening to avoid overhead\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "## import libraries\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "## import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "## MedMNIST specific library\n",
    "import medmnist\n",
    "from medmnist import BloodMNIST, INFO, Evaluator\n",
    "## local code library\n",
    "import MedMNIST_load as ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set base parameters\n",
    "Including hyper parameters and data set specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "parameter = ml.HyperParameters(learning_rate=0.001, \n",
    "                               batch_size=128, \n",
    "                               num_epochs=30, \n",
    "                               optimise=\"Adam\",\n",
    "                               loss=\"SparseCategoricalCrossentropy()\",\n",
    "                               default_activation=\"relu\")\n",
    "## use these lists to grid test hyper parameter sensitivity\n",
    "epochs_list = [10,50,100,500]                 # number of epochs to be used\n",
    "bs_list     = [32,64,128]                     # dataset batch size\n",
    "lr_list     = [1, 0.1, 0.01, 0.001, 0.0001]   # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set specifics and control (e.g. verbose) parameters\n",
    "data_flag  = 'bloodmnist'        # defines which dataset to load\n",
    "info       = INFO[data_flag]     # info about this dataset\n",
    "download   = True                # whether to download the dataset\n",
    "filebase   = \"metrics/\"\n",
    "verbose    = 1                   # to control whether additional in process information is printed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LY2oUxSh_Nbk"
   },
   "source": [
    "## Load and preprocess the BloodMNIST Data\n",
    "We load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PeebuSr_MAs",
    "outputId": "8a71101f-13e8-4381-eaff-f3bd4887b994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\johnc\\.medmnist\\bloodmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\johnc\\.medmnist\\bloodmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\johnc\\.medmnist\\bloodmnist.npz\n",
      "\n",
      "Summary metrics for train_dataset\n",
      "type: <class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n",
      "length: 94\n",
      "shape: <BatchDataset shapes: ((None, 28, 28, 3), (None, 1)), types: (tf.float64, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "# Loading the data file using common MedMINST loader\n",
    "result_set = ml.medMNIST_load(data_flag,parameter.batch_size)\n",
    "\n",
    "## check that the loader returned data correctly and then split out\n",
    "if result_set != []:\n",
    "    train_dataset = result_set[0]\n",
    "    test_dataset  = result_set[1]\n",
    "    val_dataset   = result_set[2]\n",
    "\n",
    "if verbose == 1:\n",
    "    print(\"\\nSummary metrics for train_dataset\")\n",
    "    print(\"type:\",type(train_dataset))\n",
    "    print(\"length:\",len(train_dataset))\n",
    "    print(\"shape:\",train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default activation is  relu\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 24, 24, 16)        9232      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 10, 10, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 3208      \n",
      "=================================================================\n",
      "Total params: 16,552\n",
      "Trainable params: 16,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "if verbose == 1:\n",
    "    print(\"Default activation is \",parameter.default_activation)\n",
    "    \n",
    "model = Sequential([\n",
    "    Conv2D(64, (3, 3), activation=parameter.default_activation, input_shape=(28, 28, 3)),   #1 Input\n",
    "    Conv2D(16, (3, 3), activation=parameter.default_activation),                           #2 \n",
    "    MaxPooling2D((2, 2)),                                                                   # Down-sample the feature maps\n",
    "    Conv2D(16, (3, 3), activation=parameter.default_activation),                            #2 \n",
    "    MaxPooling2D((2, 2)),                                                                   # Down-sample the feature maps\n",
    "    Flatten(),                                                                              # Flatten\n",
    "    Dense(8, activation='softmax')                                                          # Output layer for 8 types \n",
    "])\n",
    "\n",
    "print(model.summary())\n",
    "# Redirect the summary output to a string\n",
    "summary_string = io.StringIO()\n",
    "model.summary(print_fn=lambda x: summary_string.write(x + \"\\n\"))\n",
    "summary_content = summary_string.getvalue()\n",
    "summary_string.close()\n",
    "\n",
    "optimizer_choice = str(parameter.optimise)+'(learning_rate='+str(parameter.learning_rate)+')'\n",
    "optimizer        = eval(optimizer_choice)\n",
    "#loss_choice      = 'tf.keras.losses.SparseCategoricalCrossentropy()'\n",
    "#loss_choice      = 'tf.keras.losses.BinaryCrossentropy()'\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),                                                   \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics='acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 128\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "if verbose == 1:\n",
    "    print(parameter.num_epochs,parameter.batch_size)\n",
    "history = model.fit(train_dataset, \n",
    "                    validation_data=test_dataset, \n",
    "                    epochs=parameter.num_epochs, \n",
    "                    batch_size=parameter.batch_size, \n",
    "                    verbose=0)\n",
    "\n",
    "ml.graph_and_save(history,summary_content,parameter,filebase,both=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "928tj4hm_HRl",
    "LY2oUxSh_Nbk",
    "pJ0M8Nzx_Tir",
    "V_0zJR_ZAD0-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
