{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgV_pyyf--49"
   },
   "source": [
    "\n",
    "\n",
    "# AMLS Assignment Draft\n",
    "## Task B: CNN on BloodMNIST Dataset\n",
    "\n",
    "Explore CNN based classifiers on the BloodMNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "928tj4hm_HRl"
   },
   "source": [
    "## Import libraries\n",
    "The required libraries for this notebook are sklearn, copy, numpy and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZR9HIUUN-601"
   },
   "outputs": [],
   "source": [
    "## first enable autoreload during development so latest (new) version local code library is reloaded on execution \n",
    "## can be commented out when local code development not happening to avoid overhead\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "## import libraries\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "## import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, SparseCategoricalCrossentropy\n",
    "\n",
    "## removed MedMNIST specific library\n",
    "\n",
    "## local code library\n",
    "import AMLS_common as ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set base parameters\n",
    "Including hyper parameters and data set specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test cases: 24\n"
     ]
    }
   ],
   "source": [
    "parameter = ac.HyperParameters(learning_rate=0.001, \n",
    "                               batch_size=128, \n",
    "                               num_epochs=50, \n",
    "                               num_filter=16,\n",
    "                               layers=3,\n",
    "                               dropout_rate=0.2,\n",
    "                               optimise=\"Adam\",\n",
    "                               loss=\"SparseCategoricalCrossentropy()\")\n",
    "\n",
    "## set up lists and parameters\n",
    "test_list     = []\n",
    "run_list      = []\n",
    "## use these lists of values to grid test hyper parameter sensitivity                \n",
    "epochs_list   = [30]                                ## set of epochs to run for\n",
    "filter_list   = [32,64]                             ## main filter sizes to use\n",
    "bs_list       = [128]                               ## dataset batch size\n",
    "lr_list       = [0.01, 0.001]                       ## learning rates\n",
    "ly_list       = [4,5]                               ## number of covolution layers\n",
    "dr_list       = [0.1,0.2,0.3]                       ## selected dropout rates\n",
    "st_list       = [1,2]                               ## stride list\n",
    "loss_list     = ['sparse_categorical_crossentropy'] ## loss functions to use\n",
    "optimise_list = ['Adam']                            ## optimisation functions\n",
    "padding       = \"same\"\n",
    "for ep in epochs_list:\n",
    "    for bs in bs_list:\n",
    "        for lr in lr_list:\n",
    "            for fi in filter_list:\n",
    "                for ly in ly_list:\n",
    "                    for dr in dr_list:\n",
    "                        for ls in loss_list:\n",
    "                            for op in optimise_list:\n",
    "                                for st in st_list:\n",
    "                                    parameter = ac.HyperParameters(learning_rate=lr, \n",
    "                                                                   batch_size=bs, \n",
    "                                                                   num_epochs=ep, \n",
    "                                                                   num_filter=fi,\n",
    "                                                                   layers=ly,\n",
    "                                                                   dropout_rate=dr,\n",
    "                                                                   optimise=op,\n",
    "                                                                   strides=st,\n",
    "                                                                   padding=\"same\",\n",
    "                                                                   loss=ls)          \n",
    "                                test_list.append([parameter])\n",
    "## reshape parameters into a test grid that can be read using for loop\n",
    "test_grid = [hp for sublist in test_list for hp in sublist]\n",
    "print(\"test cases:\",len(test_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set specifics and control (e.g. verbose) parameters\n",
    "\n",
    "filebase   = \"metrics/\"\n",
    "verbose    = 0                   # to control whether additional in process information is printed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LY2oUxSh_Nbk"
   },
   "source": [
    "## Load and preprocess the BloodMNIST Data\n",
    "We load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PeebuSr_MAs",
    "outputId": "8a71101f-13e8-4381-eaff-f3bd4887b994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\johnc\\.medmnist\\bloodmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\johnc\\.medmnist\\bloodmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\johnc\\.medmnist\\bloodmnist.npz\n"
     ]
    }
   ],
   "source": [
    "## Loading the data file using common MedMINST loader\n",
    "data_flag  = 'bloodmnist'        # defines which dataset to load\n",
    "result_set = ac.medMNIST_load(data_flag,parameter.batch_size)\n",
    "\n",
    "## check that the loader returned data correctly and then split out\n",
    "if result_set != []:\n",
    "    train_dataset = result_set[0]\n",
    "    test_dataset  = result_set[1]\n",
    "    val_dataset   = result_set[2]\n",
    "\n",
    "if verbose == 1:\n",
    "    print(\"\\nSummary metrics for train_dataset\")\n",
    "    print(\"type:\",type(val_dataset))\n",
    "    print(\"length:\",len(val_dataset))\n",
    "    print(\"shape:\",val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.01, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=32, strides=2, padding='same', dropout_rate=0.1, layers=4, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [02:38<00:00,  5.29s/epoch, loss=0.364, acc=0.873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.01, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=32, strides=2, padding='same', dropout_rate=0.2, layers=4, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [02:36<00:00,  5.21s/epoch, loss=0.327, acc=0.886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.01, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=32, strides=2, padding='same', dropout_rate=0.3, layers=4, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [02:35<00:00,  5.17s/epoch, loss=0.411, acc=0.848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.01, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=32, strides=2, padding='same', dropout_rate=0.1, layers=5, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [02:42<00:00,  5.42s/epoch, loss=0.474, acc=0.821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.01, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=32, strides=2, padding='same', dropout_rate=0.2, layers=5, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [02:42<00:00,  5.41s/epoch, loss=0.386, acc=0.864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.01, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=32, strides=2, padding='same', dropout_rate=0.3, layers=5, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [02:56<00:00,  5.88s/epoch, loss=0.352, acc=0.866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.01, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=64, strides=2, padding='same', dropout_rate=0.1, layers=4, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [06:32<00:00, 13.08s/epoch, loss=0.266, acc=0.901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.01, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=64, strides=2, padding='same', dropout_rate=0.2, layers=4, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [07:00<00:00, 14.03s/epoch, loss=0.264, acc=0.901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.01, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=64, strides=2, padding='same', dropout_rate=0.3, layers=4, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [06:58<00:00, 13.95s/epoch, loss=0.333, acc=0.884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.01, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=64, strides=2, padding='same', dropout_rate=0.1, layers=5, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [06:38<00:00, 13.29s/epoch, loss=0.255, acc=0.914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.01, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=64, strides=2, padding='same', dropout_rate=0.2, layers=5, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [07:24<00:00, 14.80s/epoch, loss=0.258, acc=0.909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.01, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=64, strides=2, padding='same', dropout_rate=0.3, layers=5, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [07:17<00:00, 14.57s/epoch, loss=0.322, acc=0.887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.001, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=32, strides=2, padding='same', dropout_rate=0.1, layers=4, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [02:45<00:00,  5.52s/epoch, loss=0.409, acc=0.853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.001, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=32, strides=2, padding='same', dropout_rate=0.2, layers=4, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [02:54<00:00,  5.82s/epoch, loss=0.351, acc=0.868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.001, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=32, strides=2, padding='same', dropout_rate=0.3, layers=4, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [02:47<00:00,  5.57s/epoch, loss=0.356, acc=0.88] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.001, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=32, strides=2, padding='same', dropout_rate=0.1, layers=5, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [02:54<00:00,  5.81s/epoch, loss=0.341, acc=0.876]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.001, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=32, strides=2, padding='same', dropout_rate=0.2, layers=5, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [02:55<00:00,  5.86s/epoch, loss=0.337, acc=0.877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.001, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=32, strides=2, padding='same', dropout_rate=0.3, layers=5, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [02:56<00:00,  5.88s/epoch, loss=0.457, acc=0.836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.001, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=64, strides=2, padding='same', dropout_rate=0.1, layers=4, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [06:58<00:00, 13.95s/epoch, loss=0.277, acc=0.898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.001, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=64, strides=2, padding='same', dropout_rate=0.2, layers=4, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [09:43<00:00, 19.45s/epoch, loss=0.212, acc=0.926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.001, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=64, strides=2, padding='same', dropout_rate=0.3, layers=4, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [06:25<00:00, 12.87s/epoch, loss=0.247, acc=0.919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.001, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=64, strides=2, padding='same', dropout_rate=0.1, layers=5, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [06:39<00:00, 13.32s/epoch, loss=0.222, acc=0.918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.001, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=64, strides=2, padding='same', dropout_rate=0.2, layers=5, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [06:57<00:00, 13.92s/epoch, loss=0.337, acc=0.873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with HyperParameters(learning_rate=0.001, batch_size=128, num_epochs=30, optimise='Adam', loss='sparse_categorical_crossentropy', num_filter=64, strides=2, padding='same', dropout_rate=0.3, layers=5, default_activation='relu')\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress: 100%|██████████| 30/30 [07:34<00:00, 15.15s/epoch, loss=0.352, acc=0.871]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter test run complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Create instances of the dataclass from the list\n",
    "for item in test_grid:\n",
    "    ## Define the model which is then run for all learning rates in set\n",
    "    print(\"Run with\",item)\n",
    "    print(item.num_epochs)\n",
    "    \n",
    "    ## initialise tqdm callback\n",
    "    tqdm_callback = ac.TqdmEpochProgress(total_epochs=item.num_epochs)\n",
    "    \n",
    "    ## Simple CNN model to support Learning Rate analysis\n",
    "    ## added desired number of layers\n",
    "    if item.layers == 3:\n",
    "        model = Sequential([\n",
    "            Conv2D(item.num_filter*4, (3, 3), activation=item.default_activation,\\\n",
    "                   input_shape=(28, 28, 3)),                                                        ## Input layer\n",
    "            Conv2D(item.num_filter, (3, 3), activation=item.default_activation),                    ## Convolution layer \n",
    "            MaxPooling2D((2, 2)),                                                                   ## Reduce the features\n",
    "            Conv2D(item.num_filter, (3, 3), activation=item.default_activation),                    ## Another Convolution layer \n",
    "            MaxPooling2D((2, 2)),                                                                   ## Again reduce the features\n",
    "            Flatten(),                                                                              # Flatten\n",
    "            Dropout(item.dropout_rate),                                                             ## added dropout\n",
    "            Dense(8, activation='softmax')                                                          ## Output layer for 8 types \n",
    "        ])\n",
    "\n",
    "    if item.layers == 4:\n",
    "        model = Sequential([\n",
    "            Conv2D(item.num_filter*4, (3, 3), padding=item.padding,\\\n",
    "                   activation=item.default_activation,input_shape=(28, 28, 3)),                ## Input layer\n",
    "            Conv2D(item.num_filter, (3, 3), padding=item.padding,\\\n",
    "                   activation=item.default_activation),                                        ## Convolution layer \n",
    "            MaxPooling2D((2, 2),strides=item.strides),                                         ## Reduce the features\n",
    "            Conv2D(item.num_filter, (3, 3), padding=item.padding,\\\n",
    "                   activation=item.default_activation),                                        ## Another Convolution layer \n",
    "            Conv2D(item.num_filter, (3, 3), padding=item.padding,\\\n",
    "                   activation=item.default_activation),                                        ## Added Convolution layer \n",
    "            MaxPooling2D((2, 2),strides=item.strides),                                         ## Again reduce the features\n",
    "            Flatten(),                                                                         ## Flatten\n",
    "            Dropout(item.dropout_rate),                                                        ## added dropout\n",
    "            Dense(8, activation='softmax')                                                     ## Output layer for 8 types \n",
    "        ])\n",
    "        \n",
    "    if item.layers == 5:\n",
    "        model = Sequential([\n",
    "            Conv2D(item.num_filter*4, (3, 3), padding=item.padding,\\\n",
    "                   activation=item.default_activation,input_shape=(28, 28, 3)),                ## Input layer\n",
    "            Conv2D(item.num_filter, (3, 3), padding=item.padding,\\\n",
    "                   activation=item.default_activation),                                        ## Convolution layer \n",
    "            MaxPooling2D((2, 2),strides=item.strides),                                         ## Reduce the features\n",
    "            Conv2D(item.num_filter, (3, 3), padding=item.padding,\\\n",
    "                   activation=item.default_activation),                                        ## Another Convolution layer \n",
    "            Conv2D(item.num_filter, (3, 3), padding=item.padding,\\\n",
    "                   activation=item.default_activation),                                        ## Added Convolution layer \n",
    "            MaxPooling2D((2, 2),strides=item.strides),                                         ## Again reduce the features\n",
    "            Conv2D(item.num_filter, (3, 3), padding=item.padding,\\\n",
    "                   activation=item.default_activation),                                        ## Another Convolution layer \n",
    "            Conv2D(item.num_filter, (3, 3), padding=item.padding,\\\n",
    "                   activation=item.default_activation),                                        ## Added Convolution layer \n",
    "            MaxPooling2D((2, 2),strides=item.strides),                                         ## Again reduce the features\n",
    "            Flatten(),                                                                         ## Flatten\n",
    "            Dropout(item.dropout_rate),                                                        ## added dropout\n",
    "            Dense(8, activation='softmax')                                                     ## Output layer for 8 types \n",
    "        ])\n",
    "                \n",
    "    if verbose == 1:\n",
    "        print(model.summary())\n",
    "        \n",
    "    ## Redirect the summary output to a string\n",
    "    summary_string  = io.StringIO()\n",
    "    model.summary(print_fn=lambda x: summary_string.write(x + \"\\n\"))\n",
    "    summary_content = summary_string.getvalue()\n",
    "    summary_string.close()\n",
    "\n",
    "    ## Compile the model\n",
    "    model.compile(optimizer=item.optimise,                                                   \n",
    "                  loss=item.loss,\n",
    "                  metrics='acc')\n",
    "\n",
    "    ## Fit the model\n",
    "    history = model.fit(val_dataset, \n",
    "                        epochs=item.num_epochs, \n",
    "                        batch_size=item.batch_size, \n",
    "                        verbose=0,\n",
    "                        callbacks = [tqdm_callback])\n",
    "    \n",
    "    ## Save results to files\n",
    "    run_list.append(ac.hyper_process(history,summary_content,item))\n",
    "\n",
    "print(\"Hyperparameter test run complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run satisfying both smallest min_loss and largest max_acc:\n",
      "   metrics_file summary_file  min_loss   max_acc  last_loss  last_acc  \\\n",
      "21                            0.176788  0.936916   0.222018   0.91764   \n",
      "\n",
      "    var_loss   var_acc  learning_rate  batch_size  num_epochs  num_filter  \\\n",
      "21  0.229043  0.033547          0.001         128          30          64   \n",
      "\n",
      "    strides padding  dropout_rate  layers optimise  \\\n",
      "21        2    same           0.1       5     Adam   \n",
      "\n",
      "                               loss default_activation  \n",
      "21  sparse_categorical_crossentropy               relu  \n",
      "R^2 Score: 0.3415676023988541\n",
      "Mean Squared Error: 0.00023359836657046708\n",
      "\n",
      "Impact of Hyperparameters on Accuracy (from Linear Regression):\n",
      "  Hyperparameter   Coefficient\n",
      "0  learning_rate -1.534400e-01\n",
      "1     num_epochs -2.201364e-15\n",
      "2     num_filter  1.446331e-03\n",
      "3        strides  6.938894e-17\n",
      "4         layers -1.059865e-02\n",
      "5   dropout_rate -3.371952e-02\n",
      "6     batch_size  0.000000e+00\n",
      "\n",
      "Hyperparameter Importance for Accuracy (from Random Forest):\n",
      "  Hyperparameter  Importance\n",
      "2     num_filter    0.604782\n",
      "5   dropout_rate    0.178919\n",
      "4         layers    0.119240\n",
      "0  learning_rate    0.097059\n",
      "1     num_epochs    0.000000\n",
      "3        strides    0.000000\n",
      "6     batch_size    0.000000\n"
     ]
    }
   ],
   "source": [
    "best_run,run_df = ac.analyse_run(run_list,filebase)\n",
    "print(\"\\nRun satisfying both smallest min_loss and largest max_acc:\")\n",
    "print(best_run)\n",
    "\n",
    "feature_importance,coef = ac.analyse_hyperparameters(run_df)\n",
    "print(\"\\nImpact of Hyperparameters on Accuracy (from Linear Regression):\")\n",
    "print(coef)\n",
    "print(\"\\nHyperparameter Importance for Accuracy (from Random Forest):\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "928tj4hm_HRl",
    "LY2oUxSh_Nbk",
    "pJ0M8Nzx_Tir",
    "V_0zJR_ZAD0-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
