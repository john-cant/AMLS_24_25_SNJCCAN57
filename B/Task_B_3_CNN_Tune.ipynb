{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgV_pyyf--49"
   },
   "source": [
    "\n",
    "\n",
    "# AMLS Assignment\n",
    "## Task B_3: CNN Tune on BloodMNIST Dataset\n",
    "\n",
    "Explore CNN based classifiers on the BloodMNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "928tj4hm_HRl"
   },
   "source": [
    "## Import libraries\n",
    "The required libraries for this notebook are sklearn, tensorflow, numpy and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZR9HIUUN-601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "## first enable autoreload during development so latest (new) version local code library is reloaded on execution \n",
    "## can be commented out when local code development not happening to avoid overhead\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "## import libraries\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "        \n",
    "## import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, Hinge\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "## local code library\n",
    "import AMLS_common as ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set base parameters\n",
    "Including hyperparameters and environment specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.01\n",
      "kernel_size: 3\n",
      "num_epochs: 15\n",
      "optimise: Adam\n",
      "loss: sparse_categorical_crossentropy\n",
      "num_filter: 64\n",
      "strides: 1\n",
      "padding: same\n",
      "dropout_rate: 0.05\n",
      "layers: 5\n",
      "default_activation: relu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Initialize hyperparameters\n",
    "item = ac.HyperParameters.load_excel(\"param_2025_01_12_low_dropout.xlsx\")\n",
    "print(ac.HyperParameters.list_parameters(item))\n",
    "\n",
    "tqdm_callback = ac.TqdmEpochProgress(total_epochs=item.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## control (e.g. verbose) parameters\n",
    "batch_size = 128\n",
    "filebase   = \"metrics/\"          ## place to save output files\n",
    "verbose    = 1                   ## to control whether additional in process information is printed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LY2oUxSh_Nbk"
   },
   "source": [
    "## Load and preprocess the BloodMNIST Data\n",
    "We load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PeebuSr_MAs",
    "outputId": "8a71101f-13e8-4381-eaff-f3bd4887b994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\johnc\\.medmnist\\bloodmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\johnc\\.medmnist\\bloodmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\johnc\\.medmnist\\bloodmnist.npz\n",
      "\n",
      "Summary metrics for train_dataset\n",
      "type: <class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n",
      "length: 94\n",
      "shape: <BatchDataset shapes: ((None, 28, 28, 3), (None, 1)), types: (tf.float64, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "## Loading the data file using common MedMINST loader\n",
    "data_flag  = 'bloodmnist'        # defines which dataset to load\n",
    "result_set = ac.medMNIST_load(data_flag,batch_size)\n",
    "\n",
    "## check that the loader returned data correctly and then split out\n",
    "if result_set != []:\n",
    "    train_dataset = result_set[0]\n",
    "    test_dataset  = result_set[1]\n",
    "    val_dataset   = result_set[2]\n",
    "\n",
    "if verbose == 1:\n",
    "    ## print summary stats for training dataset\n",
    "    print(\"\\nSummary metrics for train_dataset\")\n",
    "    print(\"type:\",type(train_dataset))\n",
    "    print(\"length:\",len(train_dataset))\n",
    "    print(\"shape:\",train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the CNN model\n",
    "\n",
    "Define a base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default activation is  relu\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 256)       7168      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 64)        147520    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 27, 27, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 27, 27, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 26, 26, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 26, 26, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 40000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               20480512  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 4104      \n",
      "=================================================================\n",
      "Total params: 20,787,016\n",
      "Trainable params: 20,787,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Define the model\n",
    "if verbose == 1:\n",
    "    print(\"Default activation is \",item.default_activation)\n",
    "\n",
    "if item.layers == 3:\n",
    "        model = Sequential([\n",
    "            Conv2D(item.num_filter*4, kernel_size=item.kernel_size,\\\n",
    "                   padding=item.padding,activation=item.default_activation,\\\n",
    "                   input_shape=(28, 28, 3)),                                             ## Input layer with larger num filter\n",
    "            Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                   padding=item.padding,activation=item.default_activation),             ## Second part of convolution layer \n",
    "            MaxPooling2D((2, 2),strides=item.strides),                                   ## Combined with pooling\n",
    "            Dropout(item.dropout_rate),                                                  ## Initial dropout to reduce overfitting\n",
    "            Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                   padding=item.padding,activation=item.default_activation),             ## Another convolution layer \n",
    "            MaxPooling2D((2, 2),strides=item.strides),                                   ## Combined with pooling\n",
    "            Flatten(),                                                                   ## Flatten\n",
    "            Dense(item.num_filter*8,activation=item.default_activation),\n",
    "            Dropout(item.dropout_rate*2),                                                ## Added larger dropout to reduce overfitting\n",
    "            Dense(8, activation='softmax')                                               ## Output layer for 8 types \n",
    "        ])\n",
    "    \n",
    "if item.layers == 4:\n",
    "        model = Sequential([\n",
    "            Conv2D(item.num_filter*4, kernel_size=item.kernel_size,\\\n",
    "                   padding=item.padding,activation=item.default_activation,\\\n",
    "                   input_shape=(28, 28, 3)),                                             ## Input layer with larger num filter\n",
    "            Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                   padding=item.padding,activation=item.default_activation),             ## Second part of convolution layer \n",
    "            MaxPooling2D((2, 2),strides=item.strides),                                   ## Combined with pooling\n",
    "            Dropout(item.dropout_rate),                                                  ## Initial dropout to reduce overfitting\n",
    "            Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                   padding=item.padding,activation=item.default_activation),             ## Another convolution layer \n",
    "            Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                   padding=item.padding,activation=item.default_activation),             ## With added convolution layer \n",
    "            MaxPooling2D((2, 2),strides=item.strides),                                   ## Combined with pooling\n",
    "            Flatten(),                                                                   ## Flatten\n",
    "            Dense(item.num_filter*8,activation=item.default_activation),\n",
    "            Dropout(item.dropout_rate*2),                                                ## Added larger dropout to reduce overfitting\n",
    "            Dense(8, activation='softmax')                                               ## Output layer for 8 types \n",
    "        ])\n",
    "\n",
    "if item.layers == 5:\n",
    "        model = Sequential([\n",
    "            Conv2D(item.num_filter*4, kernel_size=item.kernel_size,\\\n",
    "                   padding=item.padding,activation=item.default_activation,\\\n",
    "                   input_shape=(28, 28, 3)),                                             ## Input layer with larger num filter\n",
    "            Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                   padding=item.padding,activation=item.default_activation),             ## Second part of convolution layer \n",
    "            MaxPooling2D((2, 2),strides=item.strides),                                   ## Combined with pooling\n",
    "            Dropout(item.dropout_rate),                                                  ## Initial dropout to reduce overfitting\n",
    "            Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                   padding=item.padding,activation=item.default_activation),             ## Another convolution layer \n",
    "            Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                   padding=item.padding,activation=item.default_activation),             ## With added convolution layer \n",
    "            MaxPooling2D((2, 2),strides=item.strides),                                   ## Again reduce the features\n",
    "            Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                   padding=item.padding,activation=item.default_activation),             ## Another convolution layer \n",
    "            Conv2D(item.num_filter, kernel_size=item.kernel_size,\\\n",
    "                   padding=item.padding,activation=item.default_activation),             ## With added convolution layer \n",
    "            MaxPooling2D((2, 2),strides=item.strides),                                   ## Combined with pooling\n",
    "            Flatten(),                                                                   ## Flatten\n",
    "            Dense(item.num_filter*8,activation=item.default_activation),\n",
    "            Dropout(item.dropout_rate*2),                                                ## Added larger dropout to reduce overfitting\n",
    "            Dense(8, activation='softmax')                                               ## Output layer for 8 types \n",
    "        ])\n",
    "        \n",
    "print(model.summary())\n",
    "## Redirect the summary output to a string\n",
    "summary_string = io.StringIO()\n",
    "model.summary(print_fn=lambda x: summary_string.write(x + \"\\n\"))\n",
    "summary_content = summary_string.getvalue()\n",
    "summary_string.close()\n",
    "\n",
    "optimizer_choice = str(item.optimise)+'(learning_rate='+str(item.learning_rate)+')'\n",
    "optimizer        = eval(optimizer_choice)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),                                                   \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics='acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model\n",
    "\n",
    "Fit using hyperparameters as defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  13%|█▎        | 2/15 [05:50<37:58, 175.29s/epoch, loss=0.722, acc=0.739, val_loss=0.712, val_acc=0.757]"
     ]
    }
   ],
   "source": [
    "## Fit the model\n",
    "\n",
    "history = model.fit(train_dataset, \n",
    "                    validation_data=test_dataset, \n",
    "                    epochs=item.num_epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    verbose=0,\n",
    "                    callbacks = [tqdm_callback])\n",
    "\n",
    "## output graphs and save metrics files\n",
    "ac.graph_and_save(history,summary_content,item,filebase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.graph(history,summary_content,item,skip=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    labels         = [\"basophil\",\"eosinophil\",\"erythroblast\",\"immature granulocytes\",\\\n",
    "                      \"lymphocyte\",\"monocyte\",\"neutrophil\",\"platelet\"]\n",
    "    x_test, y_test = ac.dataset_to_numpy(test_dataset)\n",
    "    prediction     = model.predict(x_test)\n",
    "    ## Convert one-hot to integer labels\n",
    "    prediction     = np.argmax(prediction, axis=1)\n",
    "    conf_matrix    = confusion_matrix(y_test, prediction)\n",
    "    print(\"Confusion Matrix:\\n\")\n",
    "    print(\"Run with\",item)\n",
    "    disp           = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels = labels)\n",
    "    disp.plot(cmap='Greens')\n",
    "    plt.xticks(rotation=45)  ## Rotate x-axis labels by 45 degrees to help readability\n",
    "    plt.show()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "928tj4hm_HRl",
    "LY2oUxSh_Nbk",
    "pJ0M8Nzx_Tir",
    "V_0zJR_ZAD0-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
